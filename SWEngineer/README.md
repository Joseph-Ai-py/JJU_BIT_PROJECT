# LangChain을 활용한 의미 분석 애플리케이션

이 문서는 LangChain, OpenAI 임베딩, Chroma 벡터 데이터베이스를 활용하여 의미 분석 애플리케이션을 구현한 Python 코드를 설명합니다. 이 애플리케이션은 Streamlit을 기반으로 동작하며, 사용자가 PDF 파일을 업로드하고 자연어 쿼리를 통해 정보를 검색할 수 있도록 설계되었습니다.

## **프로세스 설명**

### **1. 애플리케이션 초기화**
애플리케이션이 실행되면 `.env` 파일에서 환경 변수를 로드하여 OpenAI API 키를 가져옵니다. 이후 Streamlit을 통해 사용자 인터페이스(UI)가 초기화됩니다. 사용자 인터페이스는 다음과 같은 기능을 제공합니다:
- **PDF 파일 업로드**: 사용자는 사이드바에서 PDF 파일을 업로드할 수 있습니다.
- **쿼리 입력**: 사용자는 검색할 질문을 입력할 수 있습니다.
- **쿼리 전송 버튼**: 입력한 질문을 전송하여 결과를 확인할 수 있습니다.

### **2. PDF 파일 처리**
사용자가 PDF 파일을 업로드하면 애플리케이션은 다음 과정을 수행합니다:
1. **PDF 로드**: PyMuPDF를 사용하여 PDF 파일의 내용을 페이지 단위로 불러옵니다.
2. **의미 기반 텍스트 분할**: `SemanticChunker`를 사용하여 페이지 내용을 의미 단위로 분할합니다. 이 과정에서는 OpenAI 임베딩을 활용하여 의미가 비슷한 문장을 그룹화합니다.
3. **텍스트 조각 생성**: 모든 페이지에서 분할된 텍스트 조각들을 하나의 리스트로 결합합니다.

### **3. 벡터 데이터베이스 생성**
처리된 텍스트 조각을 바탕으로 Chroma 벡터 데이터베이스가 생성됩니다. 이 데이터베이스는 OpenAI 임베딩 모델을 사용하여 텍스트 조각을 벡터 형태로 변환하고, 유사성 검색을 위해 저장합니다.

### **4. 쿼리 처리 및 응답 생성**
사용자가 질문을 입력하고 전송 버튼을 누르면 애플리케이션은 다음 단계를 거쳐 응답을 생성합니다:
1. **MMR 검색**: 입력된 질문을 바탕으로 데이터베이스에서 최대 한계 적합성(MMR) 검색을 수행하여 가장 관련성 높은 문서 5개를 찾습니다.
2. **응답 생성**: 검색된 문서와 질문을 기반으로 LangChain의 `ChatPromptTemplate`과 OpenAI의 GPT 모델을 사용하여 응답을 생성합니다.
3. **결과 출력**: 생성된 응답은 Streamlit UI에 실시간으로 표시됩니다.

## **기능 요약**
- **의미 기반 텍스트 분할**: 문서의 내용을 문장 단위가 아닌 의미 단위로 분할하여 더 나은 검색 결과를 제공합니다.
- **벡터 데이터베이스 관리**: Chroma 벡터 데이터베이스를 사용하여 유사성 검색 속도와 정확도를 높입니다.
- **자연어 질문 응답**: 사용자가 입력한 자연어 질문에 대해 문서에서 관련 정보를 추출하여 응답을 생성합니다.
- **스트리밍 응답**: GPT 모델의 응답을 실시간으로 스트리밍하여 사용자에게 즉각적인 피드백을 제공합니다.

## **결과물 설명**
- 사용자는 업로드한 PDF 파일의 내용에 대해 질문을 할 수 있으며, 애플리케이션은 질문에 적합한 내용을 검색하여 결과를 출력합니다.
- 응답은 한국어로 출력되며, 의미 분석을 기반으로 한 정확한 정보를 제공합니다.

## **사용 방법**
1. **환경 변수 설정**: `.env` 파일에 OpenAI API 키를 설정합니다:
   ```
   OPENAI_API_KEY=your_openai_api_key_here
   ```
2. **필요한 라이브러리 설치**:
   ```bash
   pip install -r SWEngineer/requirements.txt
   ```
3. **애플리케이션 실행**:
   ```bash
   streamlit run SWEngineer/app.py
   ```
4. **PDF 업로드 및 쿼리 입력**: 애플리케이션 실행 후 UI를 통해 PDF 파일을 업로드하고 질문을 입력하면 결과를 확인할 수 있습니다.
