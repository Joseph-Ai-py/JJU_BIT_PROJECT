# # LangChain 기반 의미 분석 및 나이브 베이즈 분류 애플리케이션

---

### 프로젝트 개요
이 프로젝트는 LangChain, OpenAI 임베딩, Chroma 벡터 데이터베이스를 활용한 PDF 기반 의미 분석과 나이브 베이즈 분류기를 활용한 Python 애플리케이션입니다. Streamlit 기반의 웹 인터페이스를 통해 사용자에게 직관적이고 유용한 정보를 제공합니다.

---

### 주요 기능

1. **PDF 기반 의미 분석**
   - PyMuPDF로 PDF 텍스트 로드.
   - SemanticChunker를 이용해 페이지를 의미 단위로 분할.
   - Chroma 벡터 데이터베이스를 생성하여 정보 검색 속도와 정확도 향상.

2. **자연어 질문 응답**
   - OpenAI GPT를 활용해 PDF 내용에 기반한 질문에 한국어 응답 제공.
   - 실시간 스트리밍 응답으로 사용자 경험 개선.

3. **백터 데이터베이스 생성**
   -Chroma 벡터 데이터베이스를 사용하여 텍스트 청크를 벡터화하고 유사성 검색이 가능하도록 저장합니다 .

4. **MMR 검색**
   - 최대 한계 적합성(Max Marginal Relevance, MMR) 알고리즘을 사용하여 사용자의 질문과 관련성이 높은 문서를 검색합니다.

5. **실시간 응답 스트리밍**
   - 최대 한계 적합성(Max Marginal Relevance, MMR) 알고리즘을 사용하여 사용자의 질문과 관련성이 높은 문서를 검색합니다.
---

### 프로세스 설명

#### 
1. **애플리케이션 초기화**:
   - 환경 변수 로드: .env 파일에서 OpenAI API 키를 가져옵니다.
   - Streamlit UI 초기화:
       *PDF 파일 업로드.
       *자연어 쿼리 입력 및 전송 버튼 제공.

2. **PDF 파일 처리**:
      1. PDF 로드:
         -PyMuPDFLoader를 사용하여 PDF의 페이지별 텍스트를 불러옵니다.
      2. 의미 기반 텍스트 분할:
         -SemanticChunker를 사용하여 텍스트를 의미 단위로 분할합니다.
      3. 텍스트 청크 생성:
         -모든 페이지에서 분할된 텍스트를 하나의 리스트로 결합합니다.

3. **벡터 데이터베이스 생성**:
   - OpenAI 임베딩 모델을 사용하여 텍스트 청크를 벡터로 변환합니다.
   - Chroma 벡터 데이터베이스에 벡터 데이터를 저장합니다.

4. **쿼리 처리 및 응답 생성**:
    1. MMR 검색:
         -입력된 질문과 가장 관련성이 높은 문서 5개를 반환합니다..
      2. 응답 생성:
         -ChatPromptTemplate과 GPT 모델을 사용하여 질문과 문서를 기반으로           자연어 응답을 생성합니다.
      3. 결과 출력:
         -Streamlit UI에 응답을 실시간으로 표시합니다.
---

### 주요 질문 및 응답 예시

#### 
- **질문**: 이 논문의 주요 목표는 무엇인가요?
- **응답**: 
  이 논문의 주요 목표는 디지털 신기술 인재 양성을 위한 혁신 공유 대학 사업의 연구 결과를 제시하는 것입니다. 이 연구는 교육부와 한국연구재단의 지원을 받아 수행되었습니다.

#### 
- **질문**: GPQA 데이터셋에서 Search-o1의 성능은 어떤 의미를 가지나요?
- **응답**: 
  Response: Search-o1의 성능은 GPQA 데이터셋에서 검색 관련 작업의 효율성과 정확성을 평가하는 데 중요한 역할을 합니다. 이는 데이터셋 내에서 정보 검색 및 질의 응답의 정확도를 측정하는 지표로 사용될 수 있으며, 검색 알고리즘의 성능을 개선하거나 최적화하는 데 유용한 정보를 제공합니다. Search-o1의 성능은 특히 대량의 데이터에서 필요한 정보를 신속하고 정확하게 찾는 능력을 평가하는 데 중점을 둡니다.
#### 
- **질문**: 이 연구 결과가 LRM의 신뢰성에 미치는 영향은 무엇인가요?
- **응답**:
  Response: 이 연구 결과에 따르면, LRM QwQ-32B의 성능은 비추론 LLM Qwen2.5-32B와 전반적으로 유사하지만, 모든 QA 데이터셋에서 평균 EM(정확한 일치) 점수가 약간 감소한 것으로 나타났습니다(31.3에서 30.7로). 이는 LRM의 신뢰성에 약간의 부정적인 영향을 미칠 수 있음을 시사합니다. 성능이 유사하더라도, 평균 EM 점수의 감소는 LRM의 추론 능력에 대한 신뢰성을 약간 떨어뜨릴 수 있습니다.

---

### 사용 방법

1. **환경 변수 설정**
   `.env` 파일에 OpenAI API 키를 추가합니다:


2. **필수 라이브러리 설치**
필수 라이브러리를 설치합니다:
```bash
pip install -r requirements.txt
```
3. 애플리케이션 실행 Streamlit 애플리케이션을 실행합니다:
```bash
streamlit run main.py
```
4. PDF 업로드 및 질문
- Streamlit UI에서 PDF를 업로드하고 자연어로 질문을 입력해 결과를 확인하세요.


기술 스택
프레임워크: Streamlit
텍스트 처리: PyMuPDF, SemanticChunker
벡터 데이터베이스: Chroma
임베딩 모델: OpenAI Embeddings
언어 모델: OpenAI GPT

향후 개선 사항
1. PDF 외 다양한 파일 포맷 지원 (TXT, DOCX 등).
2. 다국어 질문 및 응답 처리 기능 강화.
3. 대규모 데이터 처리 속도 최적화화.

